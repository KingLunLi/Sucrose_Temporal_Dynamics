{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21eaaaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AllenLi\\AppData\\Local\\Temp\\ipykernel_29776\\2190129786.py:15: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation\n",
    "import statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import sem\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMissingTimesAndInterpolateDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns a dataframe with missing times added in and interpolated. \n",
    "    Prints out times that were added. \\n\n",
    "    DOES NOT MODIFY DataFrame IN PLACE. \\n\n",
    "    '''\n",
    "    maxTime = int(10*max(df.index))\n",
    "    minTime = int(10*min(df.index))\n",
    "    allTimes = np.round(np.linspace(minTime/10, maxTime/10, num=maxTime-minTime+1), 1)\n",
    "    unincludedTimes = set(allTimes).difference(set(df.index))\n",
    "    df2 = df.copy()\n",
    "    for t in unincludedTimes:\n",
    "        df2.loc[t] = np.nan\n",
    "    #print(\"Added\", len(unincludedTimes), \"times:\", sorted(unincludedTimes))\n",
    "    df2 = df2.sort_index()\n",
    "    df2.index.name = 'Time(s)'\n",
    "    df2.interpolate(inplace=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eece8e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45ba6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT\n",
      "itr 0\n",
      "WT\n",
      "itr 1\n",
      "WT\n",
      "itr 2\n",
      "WT\n",
      "itr 3\n",
      "WT\n",
      "itr 4\n"
     ]
    }
   ],
   "source": [
    "#set iteration \n",
    "directory= \"D:/Xiguang/sample_data/\"\n",
    "outputdirectory = \"D:/Xiguang/sample_data/\"\n",
    "dimneeded = 2\n",
    "repeats =5\n",
    "for listname in [\"WT\"]:\n",
    "    if listname == \"WT\":\n",
    "        animal_nums = [\"152\",\"153\",\"174\",\"175\",\"177\",\"180\",\"181\"]\n",
    "        \n",
    "    for number in range(repeats):\n",
    "        print(listname)\n",
    "        print(\"itr\",number)\n",
    "        for an in animal_nums:\n",
    "            it_num = 20\n",
    "            \n",
    "            #Read Lever Times\n",
    "            time_lower_range = -15\n",
    "            time_upper_range = 15\n",
    "            tot_time = (time_upper_range - time_lower_range)*10 + 1\n",
    "\n",
    "            pal = sns.color_palette('husl', 9)\n",
    "\n",
    "            animal_num = an\n",
    "            day_num = 'Day11'\n",
    "\n",
    "            file_prefix = animal_num + '_' + day_num\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # Read all times\n",
    "            standardized_path = os.path.join(directory,\"z_score/\"+ file_prefix + '_standardized.csv')\n",
    "            #standardized_path = pd.read_csv('D2#52_Day11_standardized.csv')\n",
    "            df_WD1 = pd.read_csv(standardized_path)\n",
    "            df_WD1 = df_WD1.set_index('Time(s)')\n",
    "            WD1 = addMissingTimesAndInterpolateDF(df_WD1)\n",
    "\n",
    "            # Read ALP Timing\n",
    "            alp_timing_path = os.path.join(directory, \"press_timing/\"+file_prefix + '_Lever_Press_Timing.csv')\n",
    "            ALP = pd.read_csv(alp_timing_path)\n",
    "            ALP_ori = ALP['Time(s)'].tolist()\n",
    "            ALP = ALP['Time(s)'].tolist()\n",
    "\n",
    "            #Read ILP Timing (To Select Baseline Data Outside of These Points)\n",
    "            ilp_timing_path = os.path.join(directory, \"press_timing/\"+file_prefix + '_Inactive_Lever_Press_Timing.csv')\n",
    "            ILP = pd.read_csv(ilp_timing_path)\n",
    "            ILP_ori = ILP['Time(s)'].tolist()\n",
    "            ILP = ILP['Time(s)'].tolist()\n",
    "\n",
    "            # initializing K (20s)\n",
    "            K = 10\n",
    "\n",
    "            idx = 0\n",
    "            while idx < len(ALP) - 1:\n",
    "\n",
    "                # checking for difference\n",
    "                if ALP[idx] + K > ALP[idx + 1]:\n",
    "\n",
    "                    # deleting if K closer\n",
    "                    del ALP[idx + 1]\n",
    "                else:\n",
    "                    idx += 1\n",
    "            i=0\n",
    "            while i < len(ALP):\n",
    "                if ALP[i] + time_upper_range >= WD1.index[-1]: #If ALP + 20s exceeds the length of recording time, then delete ALP\n",
    "                    del ALP[i]\n",
    "                else:\n",
    "                    i = i+1\n",
    "            i=0\n",
    "            while i < len(ALP):\n",
    "                if ALP[i] + time_lower_range <= WD1.index[0]:\n",
    "                    del ALP[i]\n",
    "                else:\n",
    "                    i = i+1    \n",
    "            ALP_all_timing_WD1 = []\n",
    "            ALP_temp = []\n",
    "\n",
    "            for y in ALP:\n",
    "                temp_list = np.around(np.arange((y + time_lower_range)*10, (y + time_upper_range)*10 + 1, 1)/10, decimals=1)[:tot_time]\n",
    "                ALP_temp.append(temp_list)\n",
    "            ALP_all_timing_WD1.append(ALP_temp)\n",
    "\n",
    "            #Create Concatenated DataFrame of ALP Activity\n",
    "            temp_concat = []\n",
    "            ALP_separate_WD1 = []\n",
    "            ALP_concat_WD1 = pd.DataFrame()\n",
    "\n",
    "            for x in range(len(ALP)):\n",
    "                temp_concat= WD1.loc[ALP_all_timing_WD1[0][x]]\n",
    "                temp_concat = temp_concat.reset_index()\n",
    "                temp_concat = temp_concat.drop(columns=['Time(s)'])\n",
    "                ALP_separate_WD1.append(temp_concat)\n",
    "                ALP_concat_WD1 = pd.concat([ALP_concat_WD1, temp_concat])\n",
    "\n",
    "            LP_WIDTH = 10\n",
    "            WIDTH_Modifier=10\n",
    "            BL_WIDTH = 5\n",
    "            while WIDTH_Modifier>= 5:\n",
    "                createInterval = lambda y: np.arange((y - LP_WIDTH)*10, (y + LP_WIDTH)*10 + 1, 1)/10\n",
    "                all_times = list(WD1.index.values)\n",
    "                non_LP_WD1 = all_times \n",
    "\n",
    "\n",
    "                \n",
    "                num_select = 2*len(ALP)\n",
    "                to_select_times_filtered = np.array(non_LP_WD1)\n",
    "                to_select_times_filtered = to_select_times_filtered[to_select_times_filtered < max(to_select_times_filtered) - time_upper_range]\n",
    "                to_select_times_filtered = to_select_times_filtered[to_select_times_filtered > min(to_select_times_filtered) - time_lower_range]\n",
    "                times = []\n",
    "                max_len_times = []\n",
    "                num_retries = 20\n",
    "                # Run the random selection multiple times and pick the largest time array \n",
    "                while len(times) != num_select and num_retries > 0: \n",
    "                    for currCount in range(num_select):\n",
    "                        if len(to_select_times_filtered) == 0: \n",
    "                            # The points selected don't space out by 20 well\n",
    "                            break\n",
    "                        time = np.random.choice(to_select_times_filtered, 1)[0]\n",
    "                        to_select_times_filtered = [t for t in to_select_times_filtered if not(t - BL_WIDTH <= time <= t + BL_WIDTH)]\n",
    "                        times.append(time)\n",
    "                    # Check for larger time selection and keep the bigger one, reset to pick again\n",
    "                    if len(times) > len(max_len_times):\n",
    "                        max_len_times = times\n",
    "                    num_retries -= 1\n",
    "                    to_select_times_filtered = np.array(non_LP_WD1)\n",
    "                    to_select_times_filtered = to_select_times_filtered[to_select_times_filtered < max(to_select_times_filtered) - time_upper_range]\n",
    "                    to_select_times_filtered = to_select_times_filtered[to_select_times_filtered > min(to_select_times_filtered) - time_lower_range]\n",
    "                    times = []\n",
    "                times = max_len_times\n",
    "                if len(times) == num_select:\n",
    "                    break\n",
    "                else:\n",
    "                    #print(\"NOT ENOUGH RANDOM TIME POINTS! Selected\", len(times), \"when we want\", num_select)\n",
    "                    if WIDTH_Modifier >5:\n",
    "                        WIDTH_Modifier = WIDTH_Modifier-1\n",
    "                    else:\n",
    "                        BL_WIDTH = BL_WIDTH-1\n",
    "                    if BL_WIDTH == 0:\n",
    "                        raise ValueError(\"BL_WIDTH = 0 for \" + file_prefix) \n",
    "                # assert(not isOverlapping(times)) # Sanity check\n",
    "            times.sort()\n",
    "\n",
    "            random.shuffle(times)\n",
    "            t = len(ALP)\n",
    "            BL1 = times[:t]\n",
    "            BL2 = times[t:]\n",
    "            df_BL_1 = WD1.loc[BL1]\n",
    "            BL1_timing = np.array(df_BL_1.index.values)\n",
    "            BL1_timing = list(BL1_timing)\n",
    "\n",
    "            BL1_all_timing = []\n",
    "            \n",
    "\n",
    "            \n",
    "            BL_temp = []\n",
    "\n",
    "            for y in BL1_timing:\n",
    "                temp_list = np.around(np.arange(round((y + time_lower_range)*10), round((y + time_upper_range)*10 + 1), 1)/10, decimals=1)\n",
    "                temp_list = [round(x, 1) for x in temp_list]\n",
    "                BL_temp.append(temp_list)\n",
    "            BL1_all_timing.append(BL_temp)\n",
    "\n",
    "\n",
    "            #Create Concatenated DataFrame of BL Activity\n",
    "            temp_concat = []\n",
    "            BL1_separate = []\n",
    "            BL1_concat = pd.DataFrame()\n",
    "\n",
    "            for x in range(len(BL1_timing)):\n",
    "                temp_concat= WD1.loc[BL1_all_timing[0][x]]\n",
    "                temp_concat = temp_concat.reset_index()\n",
    "                temp_concat = temp_concat.drop(columns=['Time(s)'])\n",
    "                BL1_separate.append(temp_concat)\n",
    "                BL1_concat = pd.concat([BL1_concat, temp_concat])\n",
    "\n",
    "            by_row_index_1 = BL1_concat.groupby(BL1_concat.index)\n",
    "            BL1_mean = by_row_index_1.mean()\n",
    "            \n",
    "            df_BL_2 = WD1.loc[BL2]\n",
    "            BL2_timing = np.array(df_BL_2.index.values)\n",
    "            BL2_timing = list(BL2_timing)\n",
    "            BL2_all_timing = []\n",
    "            BL_temp = []\n",
    "\n",
    "            for y in BL2_timing:\n",
    "                temp_list = np.around(np.arange(round((y + time_lower_range)*10), round((y + time_upper_range)*10 + 1), 1)/10, decimals=1)\n",
    "                temp_list = [round(x, 1) for x in temp_list]\n",
    "                BL_temp.append(temp_list)\n",
    "            BL2_all_timing.append(BL_temp)\n",
    "\n",
    "\n",
    "            #Create Concatenated DataFrame of BL Activity\n",
    "            temp_concat = []\n",
    "            BL2_separate = []\n",
    "            BL2_concat = pd.DataFrame()\n",
    "\n",
    "            for x in range(len(BL2_timing)):\n",
    "                temp_concat= WD1.loc[BL2_all_timing[0][x]]\n",
    "                temp_concat = temp_concat.reset_index()\n",
    "                temp_concat = temp_concat.drop(columns=['Time(s)'])\n",
    "                BL2_separate.append(temp_concat)\n",
    "                BL2_concat = pd.concat([BL2_concat, temp_concat])\n",
    "\n",
    "            by_row_index_2 = BL2_concat.groupby(BL2_concat.index)\n",
    "            BL2_mean = by_row_index_2.mean()\n",
    "            by_row_index_WD1 = ALP_concat_WD1.groupby(ALP_concat_WD1.index)\n",
    "            WD1_ALP_mean = by_row_index_WD1.mean()\n",
    "            df_concat_WD1 = pd.concat([WD1_ALP_mean,BL1_mean,BL2_mean])\n",
    "            from sklearn.pipeline import Pipeline\n",
    "\n",
    "            pipe = Pipeline([('scaler', StandardScaler()),('pca', PCA())])\n",
    "            pipe.fit(df_concat_WD1) #only call the fit method\n",
    "            pca = pipe.named_steps['pca']\n",
    "            np.savetxt(\"D:/Xiguang/PCAALP2BL/\" + animal_num + '_' + day_num + 'variance_explained_' + str(number) + '.csv', pca.explained_variance_  , delimiter=\",\")\n",
    "            projected_BL1_trials = []\n",
    "            projected_BL2_trials = []\n",
    "            projected_ALP_trials = []\n",
    "            \n",
    "            \n",
    "            for x in range(len(ALP)):\n",
    "                # project every trial using the pca fit on averages\n",
    "                proj_ALP_trial = pipe.transform(ALP_separate_WD1[x]).T\n",
    "                projected_ALP_trials.append(proj_ALP_trial)\n",
    "            # transform each trial\n",
    "            for x in range(len(BL1)):\n",
    "                # project every trial using the pca fit on averages\n",
    "                proj_BL1_trial = pipe.transform(BL1_separate[x]).T\n",
    "                projected_BL1_trials.append(proj_BL1_trial)\n",
    "\n",
    "\n",
    "\n",
    "            # transform each trial\n",
    "            for x in range(len(BL2)):\n",
    "                # project every trial using the pca fit on averages\n",
    "                proj_BL2_trial = pipe.transform(BL2_separate[x]).T\n",
    "                projected_BL2_trials.append(proj_BL2_trial)\n",
    "\n",
    "            projected_ALP_trials = np.array(projected_ALP_trials)\n",
    "            projected_ALP_trials = projected_ALP_trials[:,:,0:tot_time]\n",
    "            projected_BL1_trials = np.array(projected_BL1_trials)\n",
    "            projected_BL1_trials = projected_BL1_trials[:,:,0:tot_time]\n",
    "            projected_BL2_trials = np.array(projected_BL2_trials)\n",
    "            projected_BL2_trials = projected_BL2_trials[:,:,0:tot_time]\n",
    "            for pcwrite in range(dimneeded):\n",
    "                np.savetxt(outputdirectory + animal_num + '_' + day_num + 'trials_BL1_PC'+str(pcwrite+1)+'_it' + str(number) + '.csv', projected_BL1_trials[:,pcwrite,:] , delimiter=\",\")\n",
    "                np.savetxt(outputdirectory + animal_num + '_' + day_num + 'trials_BL2_PC'+str(pcwrite+1)+'_it' + str(number) + '.csv', projected_BL2_trials[:,pcwrite,:] , delimiter=\",\")\n",
    "                np.savetxt(outputdirectory + animal_num + '_' + day_num + 'trials_ALP_PC'+str(pcwrite+1)+'_it' + str(number) + '.csv', projected_ALP_trials[:,pcwrite,:] , delimiter=\",\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edbe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "dadd759191737a853b324efe4127839952581aeeadeb8b775b2bbbc6890a4bc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
