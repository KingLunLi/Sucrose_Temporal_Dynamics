{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493a9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxiguang\n",
    "import os\n",
    "import sys\n",
    "#sys.path.insert(0, os.path.abspath('./.local/lib/python3.8/site-packages/'))\n",
    "#sys.path.insert(0, os.path.abspath('/ihome/lwang/liw30/.local/bin'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image, ImageSequence\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06a88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ed500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3dcnn\n",
      "LSTMClassifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('3dcnn')\n",
    "class Dataset_3DCNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, data,  frames, bSampling, minD, maxD, offset):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        #self.datafile = datafile\n",
    "        self.frames=31\n",
    "        #filename=data_path+datafile\n",
    "        #print(filename)\n",
    "        data1=data\n",
    "        \n",
    "        labels=[]\n",
    "\n",
    "        index=0\n",
    "\n",
    "        count=int(data1.shape[0]/self.frames)\n",
    "        print(count)\n",
    "        page =np.ndarray(shape=(count, self.frames,20), dtype=float, order='F')\n",
    "        page_d=np.ndarray(shape=(count, self.frames,20), dtype=float, order='F')\n",
    "        index1=0\n",
    "        index2=0\n",
    "        sum1=0\n",
    "        for index, row in data1.iterrows():\n",
    "            index1=int(index/self.frames)\n",
    "            index2=int(index % self.frames)\n",
    "            if True: #skip%5==0:\n",
    "  \n",
    "                page[index1, index2,0]=row[\"nose_x\"]\n",
    "                page[index1, index2,1]=row[\"nose_y\"]\n",
    "                page[index1, index2,2]=row[\"objectA_x\"]\n",
    "                page[index1, index2,3]=row[\"objectA_y\"]\n",
    "                page[index1, index2,4]=row[\"left ear_x\"]\n",
    "                page[index1, index2,5]=row[\"left ear_y\"]\n",
    "                page[index1, index2,6]=row[\"right ear_x\"]\n",
    "                page[index1, index2,7]=row[\"right ear_y\"]\n",
    "                page[index1, index2,8]=row[\"neck_x\"]\n",
    "                page[index1, index2,9]=row[\"neck_y\"]\n",
    "                page[index1, index2,10]=row[\"middle back_x\"]\n",
    "                page[index1, index2,11]=row[\"middle back_y\"]\n",
    "                page[index1, index2,12]=row[\"middle left_x\"]\n",
    "                page[index1, index2,13]=row[\"middle left_y\"]\n",
    "                page[index1, index2,14]=row[\"middle right_x\"]\n",
    "                page[index1, index2,15]=row[\"middle right_y\"]\n",
    "                page[index1, index2,16]=row[\"tail buttom_x\"]\n",
    "                page[index1, index2,17]=row[\"tail buttom_y\"]\n",
    "                page[index1, index2,18]=row[\"tail mid_x\"]\n",
    "                page[index1, index2,19]=row[\"tail mid_y\"]\n",
    "                \n",
    "                \n",
    "            if index %self.frames==0:\n",
    "                label=row['Label2']\n",
    "                if label=='Random' :\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1) \n",
    "                    sum1=sum1+1\n",
    "        page_d=torch.from_numpy(page)\n",
    "        page_d=page_d.to(torch.float32)\n",
    "        self.data1=page_d\n",
    "        self.labels = labels      \n",
    "        print('size xlabel')\n",
    "        print(len(labels))\n",
    "        print('sum of xlabels')\n",
    "        print(sum1)\n",
    "\n",
    "        #self.transform = transform\n",
    "        #self.frames = 90\n",
    "        #print(labels)\n",
    "        #print(folders)\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "       \n",
    "\n",
    "        # Load data\n",
    "        X = self.data1[index]#.unsqueeze_(0)     # (input) spatial images\n",
    "        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        # print(X.shape)\n",
    "        return X, y\n",
    "## ---------------------- end of Dataloaders ---------------------- ##\n",
    "print('LSTMClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667a73b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "detect device\n",
      "params\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = None\n",
    "        self.hidden = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        return [t.cuda() for t in (h0, c0)]\n",
    "print('train')  \n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "   \n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "\n",
    "\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # output size = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        \n",
    "        #l1_lambda = 0.01\n",
    "        #l1_norm = sum(torch.linalg.norm(p, 1) for p in model.parameters())\n",
    "\n",
    "        #loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores\n",
    "\n",
    "print('validation')\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # to compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "    ylabel = all_y.cpu().data.squeeze().numpy()\n",
    "    ypredict = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    sensi_score = recall_score(ylabel,ypredict)\n",
    "    false_posi = recall_score(np.logical_not(ylabel),ypredict)\n",
    "    false_negat = recall_score(ylabel,np.logical_not(ypredict))\n",
    "    speci_score = recall_score(np.logical_not(ylabel),np.logical_not(ypredict))\n",
    "    \n",
    "    \n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "    print(\"True_positive: \" + str(round(sensi_score*100,4)) + \"  False_positive: \" + str(round(false_posi*100,4)))\n",
    "    print(\"False_negative: \" + str(round(false_negat*100,4)) + \"  True_negative: \" + str(round(speci_score*100,4)))\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, '3dcnn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, '3dcnn_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score, ylabel, ypredict\n",
    "\n",
    "\n",
    "# Detect devices\n",
    "print('detect device')\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print('params')\n",
    "# load UCF101 actions names\n",
    "batch_size=100\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "params1 = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test_set= Dataset_3DCNN(data_path1, test_list,60, bSampling=False, minD=30, maxD=80,offset=0)\n",
    "\n",
    "#test_loader = data.DataLoader(test_set, **params1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05549acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8744f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25636\n",
      "size xlabel\n",
      "25636\n",
      "sum of xlabels\n",
      "13050\n",
      "6467\n",
      "size xlabel\n",
      "6467\n",
      "sum of xlabels\n",
      "3306\n",
      "before model\n",
      "before opt\n",
      "Train Epoch: 1 [20000/25636 (77%)]\tLoss: 0.693166, Accu: 50.15%\n",
      "\n",
      "Test set (6467 samples): Average loss: 0.6922, Accuracy: 51.12%\n",
      "\n",
      "True_positive: 100.0  False_positive: 100.0\n",
      "False_negative: 0.0  True_negative: 0.0\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [20000/25636 (77%)]\tLoss: 0.683204, Accu: 57.15%\n",
      "\n",
      "Test set (6467 samples): Average loss: 0.6685, Accuracy: 62.83%\n",
      "\n",
      "True_positive: 71.6878  False_positive: 46.441\n",
      "False_negative: 28.3122  True_negative: 53.559\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [20000/25636 (77%)]\tLoss: 0.648748, Accu: 64.20%\n",
      "\n",
      "Test set (6467 samples): Average loss: 0.6574, Accuracy: 62.30%\n",
      "\n",
      "True_positive: 58.0762  False_positive: 33.2806\n",
      "False_negative: 41.9238  True_negative: 66.7194\n",
      "Epoch 3 model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     46\u001b[0m epoch_test_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# train, test model\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     train_losses, train_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     epoch_test_loss, epoch_test_score, ylabel1, ypredict1 \u001b[38;5;241m=\u001b[39m validation(model, device, optimizer, valid_loader)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m#epoch_test_loss1, epoch_test_score1, ylabel2, ypredict2 = validation(model, device, optimizer, test_loader)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     50\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to compute accuracy\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# y_pred != output\u001b[39;00m\n\u001b[0;32m     54\u001b[0m step_score \u001b[38;5;241m=\u001b[39m accuracy_score(y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy(), y_pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     55\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(step_score)         \u001b[38;5;66;03m# computed on CPU\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_path=\"D:/Xiguang/sample_data/DLC_data/LSTMdataset/\"\n",
    "save_model_path = \"D:/Xiguang/sample_data/DLC_data/\"\n",
    "save_result_path = \"D:/Xiguang/sample_data/DLC_data/\"\n",
    "train_data = pd.read_csv('D:/Xiguang/sample_data/DLC_data/LSTMdataset/train_set_combine-4-3_31fr.csv', index_col=False)\n",
    "valid_data = pd.read_csv('D:/Xiguang/sample_data/DLC_data/LSTMdataset/test_set_combine-4-3_31fr.csv', index_col=False)\n",
    "\n",
    "\n",
    "input_dim = 20 \n",
    "hidden_dim = 256\n",
    "layer_dim = 8\n",
    "output_dim = 2\n",
    "seq_dim = 60\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10 \n",
    "batch_size=2000\n",
    "train_set, valid_set = Dataset_3DCNN(data_path, train_data,31, bSampling=False, minD=30, maxD=80,offset=0), \\\n",
    "                       Dataset_3DCNN(data_path, valid_data,31, bSampling=False,minD=30, maxD=80,offset=0 )\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "params2 = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params2) \n",
    "iterations_per_epoch = len(train_loader)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "best_acc = 0\n",
    "patience, trials = 100, 0\n",
    "print('before model')\n",
    "model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.cuda()\n",
    "print('before opt')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 0)   # optimize all cnn parameters\n",
    "\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# load UCF101 actions names\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train, test model\n",
    "\n",
    "    train_losses, train_scores = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    epoch_test_loss, epoch_test_score, ylabel1, ypredict1 = validation(model, device, optimizer, valid_loader)\n",
    "\n",
    "    #epoch_test_loss1, epoch_test_score1, ylabel2, ypredict2 = validation(model, device, optimizer, test_loader)\n",
    "    \n",
    "    A = np.array(ylabel1)\n",
    "        \n",
    "    B = np.array(ypredict1)\n",
    "    \n",
    "    # prediction save part\n",
    "    \n",
    "    np.savetxt(save_result_path+'y_label_prediction1_'+str(epoch)+'.txt',  np.c_[A, B],fmt='%i', delimiter=',')\n",
    "        \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f04845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e2524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e614d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2150a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaaf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18809e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d25514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
